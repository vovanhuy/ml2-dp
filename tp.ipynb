{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep learning with TensorFlow\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "## To generate the name of your file, use the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp1_vo_van-huy_and_nguyen_ta-duy.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using your first and last names\n",
    "fn1 = \"Van-Huy\"\n",
    "ln1 = \"Vo\"\n",
    "fn2 = \"Ta-Duy\"\n",
    "ln2 = \"Nguyen\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp1\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "\n",
    "[0. Tensorflow basics](#basics)<br>\n",
    "[1. Handwritten digit recognition with MNIST](#mnist)<br>\n",
    "[2. A first model: softmax (or multinomial logistic) regression](#softmax)<br>\n",
    "[3. Feed-Forward Neural Network (FFNN)](#ffnn)<br>\n",
    "[4. Convolutional Neural Network](#cnn)<br>\n",
    "[5. Hand-made feed-forward network for the XOR problem](#xor)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version 1.0.0 (should be at least 0.12.1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"tensorflow version %s (should be at least 0.12.1)\" % tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basics'></a>\n",
    "# 0. Tensorflow basics\n",
    "\n",
    "The `numpy` library does some expensive operations outside Python using efficient code (Fortran, C/C++). However, switching back to python after each operation cause a big overhead because of unnecessary copies of the data. \n",
    "\n",
    "The library `tensorflow` does all the computations outside of Python: the python API is used to define a graph of operations, that will run entirely using C++ binaries. This architecture allows to get rid of the overhead. Besides, knowing the computational graph beforehand allows to parallelize and/or distribute the computation more easily. As a result, `tensoflow` can run the computations on multiple CPUs or GPUs, and on multiple servers.\n",
    "\n",
    "\n",
    "## 0.1. Operations on tensors\n",
    "\n",
    "Graphs of operations contains descriptions of symbolic variables, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: \n",
      " [[ 1.  1.  1.]\n",
      " [ 1.  1.  1.]]\n",
      "x2: \n",
      " [[ 0.]\n",
      " [ 1.]\n",
      " [ 2.]]\n",
      "x3: \n",
      " [ 0.  1.  2.]\n",
      "x4: \n",
      " [[2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "## Constant tensors\n",
    "x1 = tf.ones((2, 3), dtype=tf.float32)\n",
    "x2 = tf.constant(np.array([0, 1, 2]), shape=(3, 1), dtype=tf.float32)\n",
    "\n",
    "# equivalently\n",
    "x3 = tf.range(0, 3, 1, tf.float32)\n",
    "x4 = tf.fill((1, 3), 2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"x1: \\n\", sess.run(x1))\n",
    "    print(\"x2: \\n\", sess.run(x2))\n",
    "    print(\"x3: \\n\", sess.run(x3))\n",
    "    print(\"x4: \\n\", sess.run(x4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1: \n",
      " [[ 0.59807539  4.43660021  5.1217885 ]]\n",
      "run 2: \n",
      " [[ 0.06563938 -0.74832672 -0.66785991]]\n"
     ]
    }
   ],
   "source": [
    "## random tensors\n",
    "norm = tf.random_normal([1, 3], mean=0, stddev=2)\n",
    "\n",
    "# each time random tensors are run, new samples are generated\n",
    "with tf.Session() as sess:\n",
    "    print(\"run 1: \\n\", sess.run(norm))\n",
    "    print(\"run 2: \\n\", sess.run(norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First session\n",
      "run 1, seeded: \n",
      " [[ 0.51340485 -0.25581399  0.65199131]]\n",
      "run 2, seeded: \n",
      " [[ 1.31229651 -0.81987709 -0.71966368]]\n",
      "\n",
      "\n",
      "Second session\n",
      "run 1, seeded: \n",
      " [[ 0.51340485 -0.25581399  0.65199131]]\n",
      "run 2, seeded: \n",
      " [[ 1.31229651 -0.81987709 -0.71966368]]\n"
     ]
    }
   ],
   "source": [
    "# You can seed a random tensor to generate a repeatable sequence\n",
    "seeded_norm = tf.random_normal([1, 3], seed=1234)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"First session\")\n",
    "    print(\"run 1, seeded: \\n\", sess.run(seeded_norm))\n",
    "    print(\"run 2, seeded: \\n\", sess.run(seeded_norm))\n",
    "print(\"\\n\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Second session\")\n",
    "    print(\"run 1, seeded: \\n\", sess.run(seeded_norm))\n",
    "    print(\"run 2, seeded: \\n\", sess.run(seeded_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Input data\n",
    "To represent input data in the computation graph, we use `placeholders`. Placeholders are tensors that will be fed with data when running the session, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders example\n",
    "x = tf.placeholder(tf.float32, shape=(2, 2)) # 2 x 2 tensor\n",
    "y = tf.matmul(x, x) # dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## ERROR: will fail since x was not fed (uncomment, run the code below and look at \n",
    "##        the very long error message)\n",
    "#with tf.Session() as sess:\n",
    "#    print(sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we run the computation by feeding the contents of `rand_array` into placeholder `x`.\n",
    "Now the code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 2x2 array: \n",
      " [[ 0.64610341  0.70250021]\n",
      " [ 0.39860759  0.88679977]]\n",
      "Numpy implementation: \n",
      " [[ 0.69747153  1.0768648 ]\n",
      " [ 0.61102684  1.06643575]]\n",
      "TensorFlow implementation: \n",
      " [[ 0.6974715   1.07686472]\n",
      " [ 0.61102682  1.06643569]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    rand_array = np.random.rand(2, 2)\n",
    "    print(\"random 2x2 array: \\n\", rand_array)\n",
    "    print(\"Numpy implementation: \\n\", rand_array.dot(rand_array))\n",
    "    print(\"TensorFlow implementation: \\n\", sess.run(y, feed_dict={x: rand_array})) # Will succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3. Variables\n",
    "\n",
    "`Variables` are in-memory buffers containing tensors, i.e. they are used to save parameters used during computations: they will contain for instance the model weights of a neural network during its training. \n",
    "They maintain their state in the computation graph accross calls to `run()`, namely the contents of `variable` remain the same between different runs.\n",
    "\n",
    "In the example below we initialize a variable with a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable initialized with a constant tensor: \n",
      " [ 0.  0.  0.]\n",
      "Variable initialized with a random tensor: \n",
      " [[ 1.47003138  0.3270638  -0.18905835]\n",
      " [ 1.22427285 -0.40053752  0.87136829]]\n"
     ]
    }
   ],
   "source": [
    "# Variables must be explicitely initialized.\n",
    "# The initial value can be a tensor of any type or shape\n",
    "# Note that the default type is always float32.\n",
    "foo = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# Random tensors can be used to initialize variables\n",
    "var = tf.Variable(tf.random_normal((2, 3)))\n",
    "\n",
    "# Variable initializers must be run explicitly before \n",
    "# other ops in your model can be run\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"Variable initialized with a constant tensor: \\n\", sess.run(foo))\n",
    "    print(\"Variable initialized with a random tensor: \\n\", sess.run(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we use a `n_runs` variable which is incremented at each run. \n",
    "Note that it's **state is maintained between runs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable maintain their state across runs\n",
      "Run #1:  1\n",
      "Run #2:  2\n",
      "Run #3:  3\n"
     ]
    }
   ],
   "source": [
    "n_runs = tf.Variable(0)\n",
    "one = tf.constant(1)\n",
    "increment_n_runs = tf.assign_add(n_runs, one)\n",
    "# Remark: a more direct tf.assign_add(n_runs, 1) works also, but\n",
    "#         this allows to illustrate the use of tf.constant\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"Variable maintain their state across runs\")\n",
    "    print(\"Run #1: \", sess.run(increment_n_runs))\n",
    "    print(\"Run #2: \", sess.run(increment_n_runs))\n",
    "    print(\"Run #3: \", sess.run(increment_n_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved variable: \n",
      " [[ 0.22532335  0.12170933 -0.51565683]\n",
      " [ 0.04100075  1.59555888 -0.30819646]]\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "\n",
      "\n",
      "Model restored.\n",
      "Restored variable: \n",
      " [[ 0.22532335  0.12170933 -0.51565683]\n",
      " [ 0.04100075  1.59555888 -0.30819646]]\n"
     ]
    }
   ],
   "source": [
    "# Variables can be saved to disk during and after some computations and \n",
    "# restored later. This is particularly useful to save the weights of a \n",
    "# trained model.\n",
    "\n",
    "# A saver operation is required to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Save the variables to disk.\n",
    "    print(\"Saved variable: \\n\", sess.run(var))\n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "print(\"\\n\")\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    print(\"Restored variable: \\n\", sess.run(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Running the computations\n",
    "\n",
    "In a nutshell, `TensorFlow` works like this:\n",
    "\n",
    "* Declare a computation graph in which you define what are the inputs (`Placeholder`s), the `Variable`s, and the `Operation`s you want to perform with them.\n",
    "\n",
    "      with graph.as_default():\n",
    "          x = tf.placeholder(...)\n",
    "          W = tf.Variable(...)\n",
    "          ...\n",
    "\n",
    "* You can then run the `Operation`s on this graph as many times as you want by calling `session.run()`. A `Session` object encapsulate the environment in which `Operation` objects are executed, and `Tensor`s evaluated.\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          session.run(fetches, feed_dict=...)\n",
    "          ...\n",
    "      \n",
    "The `fetches` argument may be a single graph element, or an arbitrarily nested `list`, `tuple`, `namedtuple`, or `dict` containing graph elements at its leaves. A graph element can be one of the following types:\n",
    "\n",
    "- An `Operation`, e.g. \"run an optimizer step\". The corresponding fetched value will be None.\n",
    "- A `Tensor` (cf. examples above). The corresponding fetched value will be a numpy ndarray containing the value of that tensor.\n",
    "- and other options out of this tutorial's scope\n",
    "\n",
    "A session may own resources, such as variables, queues, and readers. \n",
    "It is important to release these resources when they are no longer required. \n",
    "To do this, either invoke the `close()` method on the session, or use the session as a context manager. \n",
    "The following two examples are equivalent:\n",
    "\n",
    "    # Using the `close()` method.\n",
    "    sess = tf.Session()\n",
    "    sess.run(...)\n",
    "    sess.close()\n",
    "\n",
    "    # Using the context manager.\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(...)\n",
    "      \n",
    "### Remark\n",
    "\n",
    "You may have noted that the tensors declared earlier were not declared explicitely as being part of a graph. \n",
    "In fact, when you don't specify a graph when declaring tensors or initializing sessions, TensorFlow will use a \"default\" graph. When working on different models, using only the default graph might be messy. \n",
    "In the following, we will declare each model in a specific graph:\n",
    "\n",
    "    some_model_graph = tf.Graph()\n",
    "\n",
    "    with some_model_graph.as_default():\n",
    "        # Declare all your tensors and operations here\n",
    "        # ...\n",
    "        \n",
    "    with tf.Session(graph=some_model_graph) as sess:\n",
    "        # Run some stuff\n",
    "        sess.run(...)\n",
    "\n",
    "Beware though, different graphs are not different namespaces, i.e. if you declare a tensor `foo` in a graph `g1` and then declare an other tensor `foo` in a graph `g2`, you might have some trouble. \n",
    "The internals of TensorFlow are out of the scope of this tutorial, but you can find more details in TensorFlow [documentation](https://www.tensorflow.org/api_docs/python/framework/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mnist'></a>\n",
    "# 1. Handwritten digit recognition with MNIST\n",
    "\n",
    "For the first part of this tutorial, we will use the [MNIST](http://yann.lecun.com/exdb/mnist) dataset.\n",
    "This dataset contains images representing handwritten digits. \n",
    "Each image is 28 x 28 pixels, and each pixel is represented by a number (gray level). \n",
    "These arrays can be flattened into vectors of 28 x 28 = 784 numbers.\n",
    "You can then see each image as a point in a 784-dimensional vector space. \n",
    "You can find interesting visualisations of this vector space [http://colah.github.io/posts/2014-10-Visualizing-MNIST/](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) (but don't spend too much time looking at the cool graphs :))\n",
    "\n",
    "## 1.1. Introduction\n",
    "\n",
    "The labels in $\\{0, 1, 2, \\ldots, 9\\}$ giving the digit on the image are be represented using one-hot encoding: labels in $\\{0, 1, 2, \\ldots, 9\\}$ are replaced by labels in $\\{ 0, 1\\}^{10}$, namely $0$ is replaced by $(1, 0, \\ldots 0)$, $1$ is replaced by $(0, 1, 0, \\ldots 0)$, $2$ is replaced by $(0, 0, 1, 0, \\ldots, 0)$, etc.\n",
    "\n",
    "MNIST data is **already normalized** to avoid numerical instability.\n",
    "Indeed, working with big floats can lead to important numerical errors. \n",
    "It is a good practice to normalize the inputs (features) so that they have zero mean and a constant (small) variance. To convince yourself of the damages that can result from numerical instability, see the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerically instable sum: 0.953674 \n",
      "Numerically stable sum: 1.000000 \n"
     ]
    }
   ],
   "source": [
    "foo = 1e9\n",
    "for i in range(int(1e6)):\n",
    "    foo += 1e-6\n",
    "foo -= 1e9\n",
    "print(\"Numerically instable sum: %f \" % foo)\n",
    "\n",
    "bar = 1\n",
    "for i in range(int(1e6)):\n",
    "    bar += 1e-6\n",
    "bar -= 1\n",
    "print(\"Numerically stable sum: %f \" % bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Download the data in the working directory\n",
    "mnist = input_data.read_data_sets(\"./mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datasets are convenient classes you can use to store data\n",
    "# when using TensorFlow. You can also use numpy.ndarrays as easily.\n",
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['count', 'index', 'test', 'train', 'validation']\n"
     ]
    }
   ],
   "source": [
    "# Structure of the dataset object\n",
    "print([elem for elem in dir(mnist) if not elem.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  55000\n",
      "Number of validation examples:  5000\n",
      "Number of test examples:  10000\n",
      "Images dimensions:  784 (i.e 28 x 28)\n",
      "Number of classes:  10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples: \", mnist.train.images.shape[0])\n",
    "print(\"Number of validation examples: \", mnist.validation.images.shape[0])\n",
    "print(\"Number of test examples: \", mnist.test.images.shape[0])\n",
    "print(\"Images dimensions: \", mnist.test.images.shape[1], \"(i.e 28 x 28)\")\n",
    "print(\"Number of classes: \", mnist.test.labels.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. A first look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we illustrate the first for elements of the training data: \n",
    "pixels grayscale of the digit and their corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2cAAADdCAYAAAA/8grrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZwAAABnQnAEAAGRAcwYAAJCB/w+ZZQBu12GM4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5399f038d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(mnist.train.images[i].reshape(28, 28), \n",
    "               interpolation=\"none\", cmap=\"gray_r\")\n",
    "    plt.text(0, 30, str(mnist.train.labels[i]), fontsize=12)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAEZCAYAAAB1rzTGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5399d73c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "n_rows = 4\n",
    "n_cols = 8\n",
    "\n",
    "for i in range(n_rows * n_cols):\n",
    "    plt.subplot(n_rows, n_cols, i+1)\n",
    "    plt.imshow(mnist.train.images[i].reshape(28, 28),\n",
    "               interpolation=\"none\", cmap=\"gray_r\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first character is a 7 digit, encoded in grayscale matrix as follows (note that we multiply again by 255):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  97  96  77 118  61   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0  90 138 235 235 235 235 235 235 251 251 248 254 245 235 190  21   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 140 251 254 254 254 254 254 254 254 254 254 254 254 254 254 254 189  23   0   0   0   0   0   0   0   0]\n",
      " [  0   0 226 254 208 199 199 199 199 139  61  61  61  61  61 128 222 254 254 189  21   0   0   0   0   0   0   0]\n",
      " [  0   0  38  82  13   0   0   0   0   0   0   0   0   0   0   0  34 213 254 254 115   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  84 254 254 234   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  84 254 254 234   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 106 157 254 254 243  51   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  25 117 228 228 228 253 254 254 254 254 240   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  68 119 220 254 254 254 254 254 254 254 254 254 142   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  37 187 253 254 254 254 223 206 206  75  68 215 254 254 117   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 113 219 254 242 227 115  89  31   0   0   0   0 200 254 241  41   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 169 254 176  62   0   0   0   0   0   0   0  48 231 254 234   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 124   0   0   0   0   0   0   0   0   0  84 254 254 166   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 139 254 238  57   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 210 250 254 168   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 242 254 239  57   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  89 251 241  86   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   5 206 246 157   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   4 117  69   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array2string((255 * mnist.train.images[0]).astype(np.int).reshape(28, 28), \n",
    "                      max_line_width=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='softmax'></a>\n",
    "# 2. A first model: softmax (or multinomial logistic) regression \n",
    "\n",
    "## 2.1. Description of the model\n",
    "\n",
    "We want to classify these pictures to predict the digit $k$ they represent for $k \\in \\{0, \\ldots, 9\\}$.\n",
    "A simple model allowing to do that is softmax regression.\n",
    "\n",
    "The idea behind this model is to produce a score for each input image $x$ using a simple linear model. \n",
    "To do so, we assume that belonging to a class $k$ (corresponding to digit $k$) can be expressed by a weigthed sum of the pixel intensities, with weights $W_{k, 1}, \\ldots, W_{k, 784}$ and to a bias $b_k$ capturing variability independent of the input:\n",
    "$$\n",
    "\\text{score}_k(x_i) = \\sum_{j=1}^{784} W_{k, j} x_j + b_k,\n",
    "$$\n",
    "These scores are sometimes called the \"logits\" in the deep learning community.\n",
    "We then use the softmax function to convert the scores into predicted probabilities $p_k$:\n",
    "$$\n",
    "p_k(x_i) = \\text{softmax}(\\text{score}_k(x_i)) = \\frac{\\exp(\\text{score}_k(x_i))}{\\sum_{k'=1}^{10}\\exp(\\text{score}_{k'}(x_i))}\n",
    "$$\n",
    "for $k=1, \\ldots, 10$.\n",
    "\n",
    "## QUESTION\n",
    "\n",
    "1. As an exercise, implement the softmax function below. Beware of the overflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # Remark: this function shouldn't be used as part of TensorFlow operations.\n",
    "    #         It is just a warming initial exercice, independent of the remaining.\n",
    "    ## TODO\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "test_input = np.array([1, .5, 0.1])\n",
    "print(softmax(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot in the next cell illustrates the effect of the softmax on an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_input = np.array([1, .5, 0.1])\n",
    "x = np.linspace(0, 5)\n",
    "y = np.array([softmax(test_input * i) for i in x])\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(x, y[:, 0], label=\"1\")\n",
    "ax.plot(x, y[:, 1], label=\"0.5\")\n",
    "ax.plot(x, y[:, 2], label=\"0.1\")\n",
    "plt.xlabel(\"multiplicative factor of the scores\")\n",
    "plt.ylabel(\"softmax\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. The computational graph for training of softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model weights $W_{k, j}$ and $b_k$ for $k=1, \\ldots, 10$ and $j=1, \\ldots, 784$, we consider the goodness-of-fit given by the negative log-likelihood.\n",
    "For the considered model, the negative log-likelihood of a sample with input $x_i \\in \\mathbb R^{784}$ and label $y_i \\in \\{0, 1\\}^{10}$, it is given by the cross-entropy between the scores $p_k(x_i)$ and the label $y_i$:\n",
    "$$\n",
    "- \\sum_{k=1}^{10} y_{i, k} \\log(p_k(x_i))\n",
    "$$\n",
    "For this first model, we will simply use stochastic gradient descent over small batches of data. It can be done easily with TensorFlow, as it will automatically and efficiently compute the gradient from your graph, then apply an optimization algorithm of your choice to perform the parameters update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "softmax_regression_graph = tf.Graph()\n",
    "\n",
    "with softmax_regression_graph.as_default():\n",
    "    # Create a placeholder of dimension `[None, 784]`, containing float32\n",
    "    # `None` means that the first dimension can be of any length\n",
    "    # This placeholder will be fed with MNIST images\n",
    "    image =  # TODO\n",
    "    \n",
    "    # Create a placeholder of dimension `[None, 10]`, containing float32\n",
    "    # to be fed with the labels\n",
    "    label =  # TODO\n",
    "\n",
    "    # Declare model parameters, ie. a tensor containing weights\n",
    "    # of dimension `[784, 10]` and a tensor containing the bias\n",
    "    # parameters of dimension `[10]`.\n",
    "    # Initialise these tensors with zeros\n",
    "    W =  # TODO\n",
    "    b =  # TODO\n",
    "\n",
    "    # Now declare the model described above\n",
    "    # you can use tf.matmul(foo, bar) to perform \n",
    "    # matrix multiplication on 2D tensors\n",
    "    # and tf.nn.softmax(baz) to compute the softmax of a tensor\n",
    "    scores =  # TODO\n",
    "    predicted_probabilities =  # TODO\n",
    " \n",
    "    # The raw formulation of cross-entropy,\n",
    "    #\n",
    "    #   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),\n",
    "    #                                 reduction_indices=[1]))\n",
    "    #\n",
    "    # can be numerically unstable.\n",
    "    #\n",
    "    # So here, use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "    # outputs of 'scores', and then average across the batch using tf.reduce_mean\n",
    "    loss =  # TODO\n",
    "    \n",
    "    # Optimization step\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Test trained model\n",
    "    correct_prediction = tf.equal(tf.argmax(predicted_probabilities, 1), tf.argmax(label, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Launching of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Size of the mini-batch\n",
    "mini_batch_size = 100\n",
    "# Number of SGD steps\n",
    "n_steps = 1000\n",
    "\n",
    "training_data = {image: mnist.train.images, label: mnist.train.labels}\n",
    "validation_data = {image: mnist.validation.images, label: mnist.validation.labels}\n",
    "test_data = {image: mnist.test.images, label: mnist.test.labels}\n",
    "\n",
    "with tf.Session(graph=softmax_regression_graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Training loop\n",
    "    for step in range(n_steps + 1):\n",
    "        # Get next mini-batch\n",
    "        batch_images, batch_labels = mnist.train.next_batch(mini_batch_size)\n",
    "        feed = {image: batch_images, label: batch_labels}\n",
    "        _, current_loss = sess.run([train_step, loss], feed_dict=feed)        \n",
    "        if step % 200 == 0:\n",
    "            print('Step %d' % step)\n",
    "            print('....Loss:     %f' % current_loss)\n",
    "            print('....Accuracy on train: %f' % sess.run(accuracy, feed_dict=training_data))\n",
    "            print('....Accuracy on validation: %f' % sess.run(accuracy, feed_dict=validation_data))\n",
    "    print('Accuracy on test: %f' % sess.run(accuracy, feed_dict=test_data))\n",
    "    # Save the weights into a numpy ndarray for plotting\n",
    "    weights = sess.run(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Visualization of the model-weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imgs = weights.reshape(28, 28, 10)\n",
    "\n",
    "F = plt.figure(1, (8, 4))\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "grid = ImageGrid(F, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(2, 5),\n",
    "                 axes_pad=0.5,\n",
    "                 add_all=True,\n",
    "                 label_mode=\"L\",\n",
    "                 cbar_mode=\"single\")\n",
    "\n",
    "vmin, vmax = imgs.min(), imgs.max()\n",
    "for i in range(10):\n",
    "    ax = grid[i]\n",
    "    im = imgs[:,:,i]\n",
    "    mappable = ax.imshow(im, interpolation=\"nearest\", vmin=vmin, vmax=vmax, cmap=\"bwr\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"%i\" % i)\n",
    "grid.cbar_axes[0].colorbar(mappable, ticks=[vmin, 0, vmax])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Some remarks\n",
    "\n",
    "You should have reached an accuracy of around 0.9 with this simple model. \n",
    "**Too easy**! You almost solved the problem using a simple softmax regression. \n",
    "Weight matrices plots show that the learned weights are consistant with the digits they should predict. \n",
    "You should be able to see rough shapes corresponding to the digits 0, 1, 2, 3, etc.\n",
    "\n",
    "MNIST is a very very **clean** dataset. Digits are rescaled, smoothed, centered, and pixel values are normalized beforehand. Let's switch to a slightly harder dataset: [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "This time, labels are letters from 'A' to 'J' (10 classes). \n",
    "These letters are taken from digital fonts instead of handwriting pictures. \n",
    "We will use a reduced amount of data to ensure a reasonable training time. \n",
    "The training set you will use has 200k labelled examples, while the validation and test sets both contain 10k labelled examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Load the notMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "\n",
    "pickle_file = './notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    train_dataset = data['train_dataset']\n",
    "    train_labels = data['train_labels']\n",
    "    valid_dataset = data['valid_dataset']\n",
    "    valid_labels = data['valid_labels']\n",
    "    test_dataset = data['test_dataset']\n",
    "    test_labels = data['test_labels']\n",
    "    del data  # hint to help the garbage collector to free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # One-hot encode the labels\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 32 examples from the notMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "n_rows = 4\n",
    "n_cols = 8\n",
    "\n",
    "for i in range(n_rows * n_cols):\n",
    "    plt.subplot(n_rows, n_cols, i+1)\n",
    "    plt.imshow(train_dataset[i].reshape(28, 28),\n",
    "               interpolation=\"none\", cmap=\"gray_r\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Training the same softmax model on notMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mini_batch_size = 100\n",
    "n_steps = 1000\n",
    "\n",
    "training_data = {image: train_dataset, label: train_labels}\n",
    "validation_data = {image: valid_dataset, label: valid_labels}\n",
    "test_data = {image: test_dataset, label: test_labels}\n",
    "\n",
    "with tf.Session(graph=softmax_regression_graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Training loop\n",
    "    for step in range(n_steps + 1):\n",
    "        # Get next batch\n",
    "        offset = (step * mini_batch_size) % (train_labels.shape[0] - mini_batch_size)\n",
    "        batch_images = train_dataset[offset:(offset + mini_batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + mini_batch_size)]\n",
    "        feed = {image: batch_images, label: batch_labels}\n",
    "        _, current_loss = sess.run([train_step, loss], feed_dict=feed)\n",
    "        if step % 100 == 0:\n",
    "            print('Step %d' % step)\n",
    "            print('....Loss:     %f' % current_loss)\n",
    "            print('....Accuracy on train: %f' % sess.run(accuracy, feed_dict=training_data))\n",
    "            print('....Accuracy on validation: %f' % sess.run(accuracy, feed_dict=validation_data))\n",
    "    print('Accuracy on test: %f' % sess.run(accuracy, feed_dict=test_data))\n",
    "    # Save the weights into a numpy ndarray for plotting\n",
    "    weights = sess.run(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is not as good as the one we had on MNIST. Let's try now more complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ffnn'></a>\n",
    "# 3. Feed-Forward Neural Network (FFNN)\n",
    "\n",
    "## 3.1. Description\n",
    "\n",
    "The softmax regression you just trained is a linear model, with 7850 parameters. \n",
    "It is easy to fit, numerically stable, but might be too simple for some tasks. \n",
    "The idea behind neural networks is to have a nonlinear model, while keeping the nice features of linear ones. \n",
    "The idea is to keep parameters into linear functions, and link these small linear model using non linear operations.\n",
    "\n",
    "A simple nonlinearity which is often used to do this is the rectified linear unit\n",
    "$\\text{ReLU}(x) = \\max(0, x)$\n",
    "\n",
    "The derivative of this function is very easy to compute, and it is parameter-free. If we stack models such as softmax regression and ReLUs, it is still very easy to compute the gradient using the chain rule, as the model is a combination of simple functions.\n",
    "\n",
    "The backpropagation algorithm allows efficient computation of complex derivatives as long as the function is made of simple blocks with simple derivatives. \n",
    "This algorithm efficiency is based on data reuse: when working with parallel architectures such as GPUs, you want to minimize communication (data transfer) as it is very time consuming in comparison to the computing time.\n",
    "\n",
    "## 3.2. Computational graph for a single hidden layer FFNN\n",
    "\n",
    "## QUESTION\n",
    "\n",
    "1. Create the graph for a fully connected feed-forward neural network with one hidden layer of size 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ffnn_graph = tf.Graph()\n",
    "\n",
    "with ffnn_graph.as_default():\n",
    "    # Create a placeholder of dimension `[None, 784]`, containing float32\n",
    "    # `None` means that the first dimension can be of any length\n",
    "    # This placeholder will be fed with MNIST images\n",
    "    image_ffnn =  # TODO\n",
    "\n",
    "    # Create a placeholder of dimension `[None, 10]`, containing float32\n",
    "    # to be fed with the labels\n",
    "    label_ffnn =  # TODO\n",
    "\n",
    "    # Declare model parameters, ie. a tensor containing weights\n",
    "    # Beware of the initialization you choose.\n",
    "    # You should not use methods of tf.contrib.layers (too easy!)\n",
    "    W_hidden =  # TODO\n",
    "    b_hidden =  # TODO\n",
    "    \n",
    "    W_out =  # TODO\n",
    "    b_out =  # TODO\n",
    "\n",
    "    # Now declare the operations of the model described above\n",
    "    # you can use tf.matmul(foo, bar) to perform \n",
    "    # matrix multiplication on 2D tensors\n",
    "    # and tf.nn.softmax(baz) to compute the softmax of a tensor\n",
    "    scores_ffnn =  # TODO\n",
    "    predicted_probabilities_ffnn =  # TODO\n",
    "\n",
    "    # The raw formulation of cross-entropy,\n",
    "    #\n",
    "    #   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),\n",
    "    #                                 reduction_indices=[1]))\n",
    "    #\n",
    "    # can be numerically unstable.\n",
    "    #\n",
    "    # So here, use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "    # outputs of 'scores', and then average across the batch using tf.reduce_mean\n",
    "    loss_ffnn =  # TODO\n",
    "    \n",
    "    # Optimization step\n",
    "    train_step_ffnn = tf.train.GradientDescentOptimizer(0.5).minimize(loss_ffnn)\n",
    "    \n",
    "    # Test trained model\n",
    "    correct_prediction_ffnn = tf.equal(tf.argmax(predicted_probabilities_ffnn, 1),\n",
    "                                       tf.argmax(label_ffnn, 1))\n",
    "    accuracy_ffnn = tf.reduce_mean(tf.cast(correct_prediction_ffnn, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Training the FFNN on notMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mini_batch_size = 100\n",
    "n_steps = 1000\n",
    "\n",
    "training_data = {image_ffnn: train_dataset, label_ffnn: train_labels}\n",
    "validation_data = {image_ffnn: valid_dataset, label_ffnn: valid_labels}\n",
    "test_data = {image_ffnn: test_dataset, label_ffnn: test_labels}\n",
    "\n",
    "with tf.Session(graph=ffnn_graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(n_steps + 1):\n",
    "        offset = (step * mini_batch_size) % (train_labels.shape[0] - mini_batch_size)\n",
    "        batch_images = train_dataset[offset:(offset + mini_batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + mini_batch_size)]\n",
    "        feed = {image_ffnn: batch_images, label_ffnn: batch_labels}\n",
    "        _, current_loss = sess.run([train_step_ffnn, loss_ffnn], feed_dict=feed)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('Step %d' % step)\n",
    "            print('....Loss:     %f' % current_loss)\n",
    "            print('....Accuracy on train: %f' % sess.run(accuracy_ffnn, feed_dict=training_data))\n",
    "            print('....Accuracy on validation: %f' % sess.run(accuracy_ffnn, feed_dict=validation_data))\n",
    "    print('Accuracy on test: %f' % sess.run(accuracy_ffnn, feed_dict=test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cnn'></a>\n",
    "# 4. Convolutional Neural Network\n",
    "\n",
    "In practice, increasing the size of hidden layers is not very effective. \n",
    "It is often a better idea to add more layers. \n",
    "Intuitively, if the phenomenon you try to learn has a hierarchical structure, adding more layers can be interpreted as a way to learn more levels of abstraction. \n",
    "For example, if you are trying to recognize objects, it is easier to express shapes from edges and objects from shapes, than to express objects from pixels. \n",
    "Thus, a good design should try to exploit this hierarchy.\n",
    "\n",
    "In particular cases, such as grid-like data (time series, images), you might want to detect a pattern which can happen in different locations of the data. \n",
    "For example, you try to detect a cat, but the cat can be in the middle or the left of the picture. \n",
    "Thus you need to build a model which is translation invariant: it is easier to learn how to recognize an object independently of its location. \n",
    "\n",
    "## 4.1. Description\n",
    "\n",
    "When two inputs might contain the same kind of information, then it is useful to share their weights and train the weights jointly for those inputs to learn statistical invariants (things that don't change much on average across time or space). \n",
    "Using this concept on images leads to convolutional neural networks (CNNs), on text, it results on recurrent neural networks (RNNs). \n",
    "When using CNNs, you set weights to a small kernel that will be used to perform a convolution across the image.\n",
    "\n",
    "The image is represented as a 3-dimensional tensor: (width, height, depth). Width and height charecterize the size of the image (eg. 28 x 28 pixels), and depth the color space (e.g. 1 for grey levels, 3 for RGB pictures since each pixel is represented by a triplet $(R,G,B)$).\n",
    "\n",
    "<img src=\"./pictures/image_tensor.png\",width=150,height=150>\n",
    "\n",
    "The convolution will map patches of this image, combined with the convolution kernel, for example\n",
    "\n",
    "$$\n",
    "\\text{output} = \\text{ReLU}(\\text{patch} \\times W + b)\n",
    "$$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./pictures/convolution_kernel.png\", width=200, height=200 alt=\"Drawing\"></td>\n",
    "        <td><img src=\"./pictures/ConvLayer.png\", width=400, height=400></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Depending on the shape of the $W$ weights tensor, the tensor resulting from the convolution can have a different depth. Note that in the context of a CNN, the \"kernel\" can be also called a \"filter\".\n",
    "\n",
    "Performing the convolution between the image and the kernel consist to move the kernel across the image, and to produce an output for each patch. \n",
    "The way you move across the image is defined by two parameters:\n",
    "\n",
    "- **Stride:** the stride is the number of pixels you are shifting each time you move your kernel during the convolution.\n",
    "- **Padding:** defines what happens when the kernel reaches a border of the image when doing the convolution. \n",
    "\"Valid\" padding means that you stop at the edge, while \"Same\" padding allows to go off the edge and pad with zeros so that the width and the height of the output and input tensors are the same.\n",
    "\n",
    "For example, a convolution with a stride $> 1$ and valid padding results in a tensor of smaller width and height. \n",
    "You can compute the size of a tensor after convolution using the following formulas:\n",
    "\n",
    "#### Valid padding\n",
    "$$\n",
    "\\text{out}_{\\text{height}} = \\bigg\\lceil \\frac{\\text{in}_{\\text{height}} - \\text{kernel}_{\\text{height}} + 1}{\\text{stride}_{\\text{vertical}}} \\bigg\\rceil \\quad \\text{ and } \\quad\n",
    "\\text{out}_{\\text{width}} = \\bigg\\lceil \\frac{\\text{in}_{\\text{width}} - \\text{kernel}_{\\text{width}} + 1}{\\text{stride}_{\\text{horizontal}}} \\bigg\\rceil\n",
    "$$\n",
    "\n",
    "#### Same padding\n",
    "$$\n",
    "\\text{out}_{\\text{height}} = \\bigg\\lceil \\frac{\\text{in}_{\\text{height}}}{\\text{strides}_{\\text{vertical}}} \n",
    "\\bigg\\rceil \\quad \\text{ and } \\quad \n",
    "\\text{out}_{\\text{width}} = \\bigg\\lceil \\frac{\\text{in}_{\\text{width}}}{\\text{strides}_{\\text{horizontal}}} \\bigg\\rceil\n",
    "$$\n",
    "\n",
    "**Example.**\n",
    "Assume the input tensor is 28x28x3 and the convolution kernel takes in 4x4x3 tensors and outputs 1x1x32 tensors (height x width x depth), i.e the kernel takes in a patch of size 4x4 and depth 3, and output a patch of size 1x1 and depth 32. To do so, the weights tensor $W$ should be 3x3x3x32 (in-height, in-width, in-depth, out-depth). \n",
    "If we are using a stride of 1, the output tensor will be 28x28x32 with same padding, and 25x25x32 with valid padding.\n",
    "Using a stride of 2, the output tensor will be 14x14x32 with same padding, and 13x13x32 with valid padding.\n",
    "\n",
    "Striding is an agressive method to reduce the image size. \n",
    "Instead, it can be a better idea to use a stride of 1 and to combine the convolution's outputs being in some neighborhood. Such an operation combining elements of a tensor is called **pooling**. \n",
    "Neighborhoods are define by the pooling window dimension (width x height) and the strides you use when moving this window across the image.\n",
    "\n",
    "**Example.**\n",
    "Max pooling aggregate several outputs in a neighborhood $N$ using a max operation: \n",
    "\n",
    "$$\n",
    "\\text{output}'_i = \\max_{j \\in N}\\text{output}_j, \\quad i \\in N.\n",
    "$$\n",
    "The formulas to compute the size of the ouput tensor are the same as for convolution padding and striding.\n",
    "\n",
    "Many successful architectures stack convolution layers in a \"pyramidal\" way: each convolution layer result in a tensor with increased depth and decreased height and width. \n",
    "Roughly, increasing the depth increases the complexity of the semantic compexity of your representation, and allows to keep the relevant information in a smaller space (height x width). \n",
    "\n",
    "## 4.2. Building a CNN with several layers\n",
    "\n",
    "## Question\n",
    "\n",
    "1. Implement a CNN having the following structure:\n",
    "\n",
    "    - input (images)\n",
    "    - 5x5 Convolution with stride 1 and same padding\n",
    "    - 2x2 max pool with stride 2 and same padding\n",
    "    - 5x5 Convolution with stride 1 and same padding\n",
    "    - 2x2 max pool with stride 2 and same padding\n",
    "    - Fully connected layer of size 1024\n",
    "    - ReLU\n",
    "    - Fully connected layer of size 10\n",
    "    - Softmax and cross-entropy as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "convNet_graph = tf.Graph()\n",
    "\n",
    "with convNet_graph.as_default():\n",
    "    # Create a placeholder of dimension `[None, 784]`, containing float32\n",
    "    # `None` means that the first dimension can be of any length\n",
    "    # This placeholder will be fed with MNIST images\n",
    "    image =  # TODO\n",
    "\n",
    "    # Create a placeholder of dimension `[None, 10]`, containing float32\n",
    "    # to be fed with the labels\n",
    "    label =  # TODO\n",
    "    \n",
    "    # Reshape the image as a 28 x 28 X 1 tensor\n",
    "    x_image = tf.reshape(image, [-1, 28, 28, 1])\n",
    "    # If one component of shape is the special value -1, \n",
    "    # the size of that dimension is computed so that the\n",
    "    # total size remains constant. At most one component\n",
    "    # of shape can be -1.\n",
    "\n",
    "    # 5x5 convolution layer, from depth 1 to depth 32\n",
    "    # Get a look at tf.nn.conv2d documentation to shape your weights properly\n",
    "    W_conv1 =  # TODO\n",
    "    b_conv1 =  # TODO\n",
    "    # Perform the convolution and then apply a ReLU\n",
    "    h_conv1 =  # TODO\n",
    "    \n",
    "    # Perform a 2x2 max pool, see tf.nn.max_pool\n",
    "    h_pool1 =  # TODO\n",
    "\n",
    "    # 5x5 convolution, from depth 32 to depth 64\n",
    "    W_conv2 =  # TODO\n",
    "    b_conv2 =  # TODO\n",
    "\n",
    "    # Perform the convolution and then apply a ReLU\n",
    "    h_conv2 =  # TODO\n",
    "    \n",
    "    # 2x2 max pool, see tf.nn.max_pool\n",
    "    h_pool2 =  # TODO\n",
    "\n",
    "    # Fully connected layer of 1024 neurons\n",
    "    number_of_nodes_of_h_pool2_layer =  # TODO    # height x width x depth of h_pool2\n",
    "    W_fc1 =  # TODO # Weights of the fully connected layer\n",
    "    b_fc1 =  # TODO # Biases of the fully connected layer\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, number_of_nodes_of_h_pool2_layer])\n",
    "    # Apply the ReLU after the matrix multiplication (cf. what you did for softmax)\n",
    "    h_fc1 =  # TODO\n",
    "    \n",
    "    # Dropout with keep_prob (placeholder for the argument)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # Fully connected layer into the 10 labels\n",
    "    W_fc2 =  # TODO # Weights of the fully connected layer\n",
    "    b_fc2 =  # TODO # Biases of the fully connected layer\n",
    "\n",
    "    scores = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    # Compute cross-entropy with softmax loss, cf. what you did for the previous models\n",
    "    loss =  # TODO\n",
    "    \n",
    "    # Optimization algorithm: ADAM, see https://arxiv.org/abs/1412.6980\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "    \n",
    "    # Compute performance\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(label,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Training of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mini_batch_size = 100\n",
    "n_steps = 750\n",
    "\n",
    "training_data = {image: train_dataset, label: train_labels, keep_prob: 1}\n",
    "validation_data = {image: valid_dataset, label: valid_labels, keep_prob: 1}\n",
    "test_data = {image: test_dataset, label: test_labels, keep_prob: 1}\n",
    "\n",
    "with tf.Session(graph=convNet_graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Training loop\n",
    "    for step in range(n_steps):\n",
    "        # Get next batch\n",
    "        offset = (step * mini_batch_size) % (train_labels.shape[0] - mini_batch_size)\n",
    "        batch_images = train_dataset[offset:(offset + mini_batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + mini_batch_size)]\n",
    "        feed = {image: batch_images, label: batch_labels, keep_prob: 0.5}\n",
    "        _, current_loss = sess.run([train_step, loss], feed_dict=feed)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('Step %d' % step)\n",
    "            print('....Loss:     %f' % current_loss)\n",
    "            print('....Accuracy on train (mini batch): %f' % sess.run(accuracy, feed_dict=feed))\n",
    "            print('....Accuracy on validation: %f' % sess.run(accuracy, feed_dict=validation_data))\n",
    "    print('Accuracy on test: %f' % sess.run(accuracy, feed_dict=test_data))\n",
    "    # Save the weights into a numpy ndarray for plotting\n",
    "    weights = sess.run([W_conv1, h_conv1, h_pool1, W_conv2, h_conv2, h_pool2, W_fc1, W_fc2],\n",
    "                        feed_dict={image: train_dataset[0].reshape((1,784)),\n",
    "                                   label: train_labels[0].reshape((1,10)),\n",
    "                                   keep_prob: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on test: 0.949200 avec 1000 passes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Visualization of the filters\n",
    "\n",
    "You can get an idea of what the filters are doing to the image by visualizing the tensors produced during forward propagation. The first convolutional layer tend to accentuate contours, while the second one picks salient details of the image. The fully connected layers' job is then simplified as they have access to higher level abstractions to identify the letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Image going through the forward propagation\n",
    "plt.imshow(train_dataset[0].reshape((28,28)), cmap=\"gray_r\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Image representation after the first convolution and ReLU\n",
    "imgs = np.squeeze(weights[1])\n",
    "\n",
    "F = plt.figure(1, (8, 12))\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "grid = ImageGrid(F, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(8, 4),\n",
    "                 axes_pad=0.5,\n",
    "                 add_all=True,\n",
    "                 label_mode=\"L\",\n",
    "                 cbar_mode=\"single\"\n",
    "                 )\n",
    "\n",
    "vmin, vmax = imgs.min(), imgs.max()\n",
    "for i in range(32):\n",
    "    ax = grid[i]\n",
    "    im = imgs[:,:,i]\n",
    "    mappable = ax.imshow(im, interpolation=\"nearest\", vmin=vmin, vmax=vmax, cmap=\"bwr\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"filter %i\" % i)\n",
    "grid.cbar_axes[0].colorbar(mappable, ticks=[vmin, 0, vmax])\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Image representation after the first convolution and ReLU and max pool\n",
    "imgs = np.squeeze(weights[2])\n",
    "\n",
    "F = plt.figure(1, (8, 12))\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "grid = ImageGrid(F, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(8, 4),\n",
    "                 axes_pad=0.5,\n",
    "                 add_all=True,\n",
    "                 label_mode=\"L\",\n",
    "                 cbar_mode=\"single\"\n",
    "                 )\n",
    "\n",
    "vmin, vmax = imgs.min(), imgs.max()\n",
    "for i in range(32):\n",
    "    ax = grid[i]\n",
    "    im = imgs[:,:,i]\n",
    "    mappable = ax.imshow(im, interpolation=\"nearest\", vmin=vmin, vmax=vmax, cmap=\"bwr\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"filter %i\" % i)\n",
    "grid.cbar_axes[0].colorbar(mappable, ticks=[vmin, 0, vmax])\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Image representation after the second convolution and ReLU\n",
    "imgs = np.squeeze(weights[4])\n",
    "\n",
    "F = plt.figure(1, (8, 30))\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "grid = ImageGrid(F, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(16, 4),\n",
    "                 axes_pad=0.5,\n",
    "                 add_all=True,\n",
    "                 label_mode=\"L\",\n",
    "                 cbar_mode=\"single\"\n",
    "                 )\n",
    "\n",
    "vmin, vmax = imgs.min(), imgs.max()\n",
    "for i in range(64):\n",
    "    ax = grid[i]\n",
    "    im = imgs[:,:,i]\n",
    "    mappable = ax.imshow(im, interpolation=\"nearest\", vmin=vmin, vmax=vmax, cmap=\"bwr\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"filter %i\" % i)\n",
    "grid.cbar_axes[0].colorbar(mappable, ticks=[vmin, 0, vmax])\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Image representation after the second convolution and ReLU and max pool\n",
    "imgs = np.squeeze(weights[5])\n",
    "\n",
    "F = plt.figure(1, (8, 30))\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "grid = ImageGrid(F, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(16, 4),\n",
    "                 axes_pad=0.5,\n",
    "                 add_all=True,\n",
    "                 label_mode=\"L\",\n",
    "                 cbar_mode=\"single\"\n",
    "                 )\n",
    "\n",
    "vmin, vmax = imgs.min(), imgs.max()\n",
    "for i in range(64):\n",
    "    ax = grid[i]\n",
    "    im = imgs[:,:,i]\n",
    "    mappable = ax.imshow(im, interpolation=\"nearest\", vmin=vmin, vmax=vmax, cmap=\"bwr\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"filter %i\" % i)\n",
    "grid.cbar_axes[0].colorbar(mappable, ticks=[vmin, 0, vmax])\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='xor'></a>\n",
    "# 5. Hand-made feed-forward network for the XOR problem\n",
    "\n",
    "So far, `tensorflow` did all the work. A central tool for the training of neural networks is the backpropagation, which is the technique used to compute the gradient with respect to the model weights.\n",
    "As an exercice, in order to really understand the way backpropagation works, we propose to code it by yourself, in order to learn the XOR (exclusive OR) function.\n",
    "\n",
    "## QUESTION\n",
    "\n",
    "1. Based on the backprop algorithm presented in [The Deep Learning Book, chapter 6](http://www.deeplearningbook.org/contents/mlp.html), pages 213-213, implement a simple NN using numpy to learn the XOR function with the following fully connected NN:\n",
    "\n",
    "![NN_architecture](./pictures/XOR_NN.png)\n",
    "\n",
    "* Input layer of size 2 (size of a sample of data)\n",
    "* One fully connected hidden layer of size 3 with sigmoid activation\n",
    "* Output layer of size 2 using softmax activation\n",
    "* Use cross-entropy as loss function\n",
    "* Train the NN using gradient descent\n",
    "\n",
    "Using this network, you can learn the XOR function with 10000 GD iterations and a learning rate of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function (overflow-proof)\"\"\"\n",
    "    ## TODO\n",
    "    \n",
    "    ## END TODO\n",
    "\n",
    "# Activation function derivative\n",
    "def grad_sigmoid(t):\n",
    "    ## TODO\n",
    "    \n",
    "    ## END TODO\n",
    "\n",
    "print(check_grad(sigmoid, grad_sigmoid, np.array([-.7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction and loss functions\n",
    "def softmax(x):\n",
    "    ## TODO\n",
    "    \n",
    "    ## END TODO\n",
    "\n",
    "def cross_entropy(y, p):\n",
    "    ## TODO\n",
    "    \n",
    "    ## END TODO\n",
    "\n",
    "# cross_entropy(y, softmax(z))\n",
    "def cross_entropy_with_softmax(y, z):\n",
    "    ## TODO\n",
    "    \n",
    "    ## END TODO\n",
    "\n",
    "def grad_cross_entropy_with_softmax(y, z):\n",
    "    ## TODO\n",
    "    \n",
    "    ## END TODO\n",
    "\n",
    "y = np.array([0, 1])\n",
    "f = lambda x: cross_entropy_with_softmax(y, x)\n",
    "f_prime = lambda x: grad_cross_entropy_with_softmax(y, x)\n",
    "print(check_grad(f, f_prime, np.array([.7, .3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])\n",
    "m = data.shape[0]\n",
    "\n",
    "learning_rate = .5\n",
    "\n",
    "\n",
    "# Initialize the variables (weights + biases) for the hidden and output layers\n",
    "# Tip: you should initialize those weigths with random numbers between 0 and 1\n",
    "# Hidden layer\n",
    "W_1 = # TODO                 # Hidden layer of size 3 weights\n",
    "b_1 = # TODO                 # Hidden layer bias terms\n",
    "# Output layer\n",
    "W_2 = # TODO                 # Output layer of size 3 weights\n",
    "b_2 = # TODO                 # Output layer bias terms\n",
    "    \n",
    "# Learning\n",
    "for epoch in range(10000):\n",
    "    # initialize / reset gradients with zeros\n",
    "    grad_W_2 = # TODO\n",
    "    grad_W_1 = # TODO\n",
    "    grad_b_1 = # TODO\n",
    "    grad_b_2 = # TODO\n",
    "    # loop over samples\n",
    "    loss = 0\n",
    "    for i in range(m):\n",
    "        #### forward pass\n",
    "        # input layer\n",
    "        h_0 = data[i].reshape((2,1)) # WARNING: make sure the sample's shape is (2,1)\n",
    "        # hidden layer pass\n",
    "        a_1 = # TODO\n",
    "        h_1 = # TODO\n",
    "        # output layer pass\n",
    "        a_2 = # TODO\n",
    "        h_2 = # TODO\n",
    "        \n",
    "        #### Compute loss for the current sample\n",
    "        loss += # TODO\n",
    "\n",
    "        #### backward pass\n",
    "        # compute neuron errors and update gradient value\n",
    "        # for the output layer\n",
    "        g = # TODO\n",
    "        grad_b_2 += # TODO\n",
    "        grad_W_2 += # TODO\n",
    "        # then for the input layer\n",
    "        g = # TODO\n",
    "        grad_b_1 += # TODO\n",
    "        grad_W_1 += # TODO\n",
    "        \n",
    "    # Update the weights using gradient descent\n",
    "    b_1 -= # TODO\n",
    "    W_1 -= # TODO\n",
    "    b_2 -= # TODO\n",
    "    W_2 -= # TODO\n",
    "    if epoch % 1000 == 0:\n",
    "        print(\"loss at epoch %i: %.4f\" %(epoch, loss))\n",
    "\n",
    "\n",
    "def predict(sample_data):\n",
    "    h_0 = sample_data.reshape((2,1)) # WARNING: make sure the sample's shape is (2,1)\n",
    "    # forward pass to get the NN predictions, same one as the one used for the learning\n",
    "    # Hidden layer\n",
    "    a_1 = # TODO\n",
    "    h_1 = # TODO\n",
    "    # Output layer\n",
    "    a_2 = # TODO\n",
    "    h_2 = # TODO\n",
    "    return h_2.T\n",
    "\n",
    "m = data.shape[0]\n",
    "for i in range(m):\n",
    "    print(data[i], labels[i], np.round(predict(data[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "You can try to improve your results using some forms of regularization (early stopping, dropout, etc.) to improve even further your results.\n",
    "You can also play with some other dataset, such as CIFAR-10, but computations are wayyy longer and require the use of a large GPU to accelerate things."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
